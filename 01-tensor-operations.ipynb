{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring PyTorch Tensor\n",
    "\n",
    "### Short exploration and explanation of PyTorch Tensor function\n",
    "\n",
    "An short introduction about PyTorch and about the chosen functions. \n",
    "- ``torch.logspace``\n",
    "- ``torch.argmin``\n",
    "- ``torch.cat``\n",
    "- ``torch.split``\n",
    "- ``torch.mean``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch and other required modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 1 - torch.logspace\n",
    "\n",
    "The torch.logspace function returns a 1-D tensor of ``steps`` point logarithmically spaced with ``base`` base between base<sup>start</sup> and base<sup>end</sup>.\n",
    "\n",
    "\n",
    "**Note**: ``torch.linspace`` is a linear spacing and ``torch.logspace`` is logarithmic spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  1,  3, 10], dtype=torch.int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "torch.logspace(start=-1, end=1, steps=5, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example returns a tensor with 5 integers from 10<sup>-1</sup> to 10<sup>1</sup> with logarithmically spaced. Here, the values are rounded since the desired data type is set to an Integer (torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1000,  0.3162,  1.0000,  3.1623, 10.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2 -working\n",
    "torch.logspace(start=-1, end=1, steps=5, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, data type is changed to float (torch.float64) and now the points appear to be logarithmically spaced with given ``base``(default 10.0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1]], dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dtype Float does not match dtype of out parameter (Int)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-76b4fa9f539c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: dtype Float does not match dtype of out parameter (Int)"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking (to )\n",
    "x = torch.ones((1,4), dtype=torch.int32)\n",
    "print(x)\n",
    "torch.logspace(-1, 1, steps=5, out=x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch.logspace function also has an out parameter which replaces the values of torch.logspace function given the desired tensor.Here we are getting RuntimeError because our **out** parameter is a tensor initialized to store Integers while the resulting tensor from the torch.linspace function is initialized to store Float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1250e-02, 6.7504e-02, 1.4582e-01, 3.1498e-01, 6.8040e-01, 1.4697e+00,\n",
      "        3.1748e+00, 6.8580e+00, 1.4814e+01, 3.2000e+01])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.logspace(start = -5, end = 5, steps = 10, base = 2)\n",
    "print(x)\n",
    "\n",
    "# visualization of torch.logspace\n",
    "plt.plot(x.numpy(), np.zeros(x.numpy().shape), color='green', marker=\"o\")\n",
    "plt.title(\"torch.logspace\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows how the values are logarithmically spaced.\n",
    "\n",
    "This function is useful when you need a 1-D tensor with logarithmically spaced points with base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2 - torch.argmin\n",
    "\n",
    "The torch.argmin function returns the indices of the minimum value of all elements in the ``input`` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0718,  1.5564, -0.8455, -0.4836,  1.8471]])\n",
      "Index of minimum value: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,5)\n",
    "print(a)\n",
    "print(\"Index of minimum value:\", torch.argmin(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the torch.argmin function applied to a (1,5) vector. We can see that the minimum value in vector is at index 0 and value at given index is -1.0718"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7942,  1.2174,  1.6016],\n",
      "        [ 1.0441,  1.1672,  0.4306],\n",
      "        [-0.3423, -0.0643, -2.3043]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "b = torch.randn(3,3)\n",
    "print(b)\n",
    "torch.argmin(b, dim=0) # 1-> over row, 0 -> over column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are using ``dim`` parameter in torch.argmin function. We can see that our output is a tensor list which contains index of minimum value along the specified dimension 0 (over column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmin(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-307d0c42db6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: argmin(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking (to illustrate when it breaks)\n",
    "c = [1,2,3,4]\n",
    "print(c)\n",
    "torch.argmin(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that providing normal list to a torch.argmin function return a TypeError because the input is a normal list not a Tensor list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function should be used when you need to find the position of minimum values in your tensor. It also has capability of finding index of minimum values over different dimension.\n",
    "\n",
    "Check also: **torch.min()**, **torch.argmax()** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3 - torch.cat\n",
    "\n",
    "The torch.cat function concatenates the given sequence of tensors in the given dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5482, -0.2348],\n",
      "        [-0.3552, -1.3314]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5482, -0.2348],\n",
       "        [-0.3552, -1.3314],\n",
       "        [-1.5482, -0.2348],\n",
       "        [-0.3552, -1.3314]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "x = torch.randn(2,2)\n",
    "print(x)\n",
    "torch.cat((x,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the function torch.cat concatenates the given matrix x over rows (default). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5482, -0.2348, -1.5482, -0.2348, -1.5482, -0.2348],\n",
      "        [-0.3552, -1.3314, -0.3552, -1.3314, -0.3552, -1.3314]])\n",
      "---------\n",
      "tensor([[-1.5482, -0.2348],\n",
      "        [-0.3552, -1.3314],\n",
      "        [-1.5482, -0.2348],\n",
      "        [-0.3552, -1.3314],\n",
      "        [-1.5482, -0.2348],\n",
      "        [-0.3552, -1.3314]])\n"
     ]
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "cat_col = torch.cat((x,x,x), 1) # over column\n",
    "print(cat_col)\n",
    "\n",
    "print(\"---------\")\n",
    "\n",
    "cat_row = torch.cat((x,x,x), 0) # over rows\n",
    "print(cat_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that using a dim parameter in torch.cat function concatenates the tensor in different dimensions. Here, cat_col tensor concatenates the tensor over column and cat_row tensor concatenates the tensor over rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2783, -1.5277],\n",
      "        [-1.2865, -1.1378]])\n",
      "tensor([[ 0.7400, -0.0190, -0.9407],\n",
      "        [ 0.6897,  0.8063,  1.1646],\n",
      "        [-1.4199, -0.5748,  0.0982]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Got 2 and 3 in dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3cd150ad6206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Got 2 and 3 in dimension 1"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking (to illustrate when it breaks)\n",
    "a = torch.randn(2,2)\n",
    "b = torch.randn(3,3)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "torch.cat((a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in this example the torch.cat function is applied to a(2,2) matrix and b(3,3) matrix and we are getting RuntimeError because the dimension of matrix is not matching across column. if we change the dimension of b to (3,2) it will work fine.\n",
    "\n",
    "> **Note**: All tensors must either have the same shape (except in the concatenating dimension) or be empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful when we want to concatenate large amount of data in one big matrix for large processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 4 - torch.split\n",
    "\n",
    "The torch.split function splits a longer tensor into smaller tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2]), tensor([3, 4]), tensor([5, 6]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "x = torch.tensor([1,2,3,4,5,6])\n",
    "torch.split(x, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in this example the torch.split function is applied to a tensor vector x which splits the vectors by given size of chunk i.e. 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3133,  0.0467, -0.1075,  0.0705],\n",
      "        [-0.7927,  0.3365, -2.1977,  0.0273],\n",
      "        [ 0.5852, -0.2618, -0.2891, -0.7143],\n",
      "        [-1.0531, -0.4570,  0.8493,  0.5974]])\n",
      "-----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.3133,  0.0467],\n",
       "         [-0.7927,  0.3365],\n",
       "         [ 0.5852, -0.2618],\n",
       "         [-1.0531, -0.4570]]), tensor([[-0.1075,  0.0705],\n",
       "         [-2.1977,  0.0273],\n",
       "         [-0.2891, -0.7143],\n",
       "         [ 0.8493,  0.5974]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "y = torch.randn(4,4)\n",
    "print(y)\n",
    "print(\"-----\")\n",
    "torch.split(y, 2, dim=1) # over column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in this example the torch.split function is applied to a matrix of (4,4) with dim parameter set to 1 (over column). It return the two chunks of (4,2) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5], dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "split_with_sizes expects split_sizes to sum exactly to 5 (input tensor's size at dimension 0), but got split_sizes=[3, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c06429713fe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(tensor, split_size_or_sections, dim)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# split_size_or_sections. The branching code is in tensor.py, which we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# call here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_size_or_sections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# equivalent to itertools.product(indices)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_with_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_with_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: split_with_sizes expects split_sizes to sum exactly to 5 (input tensor's size at dimension 0), but got split_sizes=[3, 1]"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking (to illustrate when it breaks)\n",
    "z = torch.tensor([1,2,3,4,5], dtype=torch.int32)\n",
    "print(z)\n",
    "torch.split(z, [3,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, our tensor z is a integer tensor. Applying the split function to the tensor results in a RuntimeError because the split_size_or_selection paramter is not set correctly. Here split_size_or_selections paramter is a list and this will work only when the sum of values in a list is exaclty equal to length of the given list.\n",
    "\n",
    "For example: len(z) = 5. therefore split_size_or_selection parameter should be sum exactly 5 i.e., our paramter will have ([3,2], [1,4], [5,0]) values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restructuring the input tensors into smaller tensors not only fastens the calculation process, but also helps in distributed computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 5 - torch.mean\n",
    "\n",
    "The torch.mean function calculates the mean or average of your tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "x = torch.tensor([2,2,2,2], dtype=torch.float32)\n",
    "print(x)\n",
    "torch.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in this example the torch.mean function is applied to a tensor value which is just adding all the numbers divded by the length of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9595, -0.2954,  0.1083],\n",
      "        [ 0.2088,  0.7644,  1.0501],\n",
      "        [-0.7986, -0.8966, -0.4241]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.5164, -0.1426,  0.2447])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "y = torch.randn((3,3))\n",
    "print(y)\n",
    "torch.mean(y, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in this example the torch.mean function is applied to a (3,3) matrix except the dim parameter is set to 0. This is specifying that we want the average over all rows (for each column). We can set dim to 1, if we want the average over all columns (for each row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can only calculate the mean of floating types. Got Long instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8e1a9d110577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 - breaking (to illustrate when it breaks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Can only calculate the mean of floating types. Got Long instead."
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking (to illustrate when it breaks)\n",
    "z = torch.tensor([2,2,2,2])\n",
    "torch.mean(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In thi example, our tensor z is a integer data type. Applying the mean function to this integer tensor results in a RuntimeError beacuse you cannot get the mean of integer data types using torch.mean. That is why the RuntimeError syas it can only calculate the mean of floating types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function should be use for calculating the mean of your tensor, you can also specify which dimension to calculate the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we only go over 5 of many different tensor functions that are in PyTorch tensor library. To learn more, you can visit:\n",
    "\n",
    "[PyTorch Documentation](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Links\n",
    "Provide links to your references and other interesting articles about tensors\n",
    "* Official documentation for `torch.Tensor`: https://pytorch.org/docs/stable/tensors.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
